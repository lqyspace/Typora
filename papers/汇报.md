> 每位研一同学选定2～3篇参考文献（近三年顶级），作为开题和硕士论文的重要参考。2. 从文献中抽取出研究的问题，下次例会详细汇报。要给出问题的形式化定义（即数学模型）、具体案例或应用场景（即举例说明该问题的研究意义）。3. 一个规范：在ppt或文档中提及论文时，要按照参考文献的格式给出完整信息。

# RlCard与推荐的映射

## 以斗地主为例

- State（状态空间）：在游戏的每个决策点上，每个玩家以自己的角度能够观察到的当前状态的所有信息。以下以 **字典** 的形式呈现状态结构。

  ![image-20220513134837083](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202205181725667.png)



- Action（动作空间）：54张牌，根据不同的牌的组合可有27472种组合。在新版的RlCard里，出牌的动作空间有27472种，分别编码0-27471。在旧版的RlCard里，出牌的动作空间被抽象成309种，抽象过程：例如：“33344”被抽象为“333 **”。
- Action Encoding：54-d one-hot vector

![image-20220513135040258](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202205181725488.png)



- 收益规则：如果地主先出完手中所有牌，获胜并得1分。如果任一平民出完所有牌，获胜得1分。



> **以斗地主为例**：

> 每位研一同学选定2～3篇参考文献（近三年顶级），作为开题和硕士论文的重要参考。2. 从文献中抽取出研究的问题，下次例会详细汇报。要给出问题的形式化定义（即数学模型）、具体案例或应用场景（即举例说明该问题的研究意义）。3. 一个规范：在ppt或文档中提及论文时，要按照参考文献的格式给出完整信息。

**参考文献：**

[1] Anwaar M U, Han Z, Arumugaswamy S, et al. Metapath-and Entity-aware Graph Neural Network for Recommendation[J]. arXiv preprint arXiv:2010.11793, 2020.

[2] Ning W, Cheng R, Shen J, et al. Reinforced Meta-path Selection for Recommendation on Heterogeneous Information Networks[J]. arXiv preprint arXiv:2112.12845, 2021.

[3] Zha D, Lai K H, Cao Y, et al. Rlcard: A toolkit for reinforcement learning in card games[J]. arXiv preprint arXiv:1910.04376, 2019.



1、卡牌action与电影一一对应：

|                 |                 卡牌游戏                 |                         电影推荐                         |
| :-------------: | :--------------------------------------: | :------------------------------------------------------: |
| Agent（智能体） |                                          |                                                          |
|                 |            地主(the landlord)            |         第三方（或一个可以给用户做推荐的智能体）         |
|   Env（环境）   |                                          |                                                          |
|                 |    一场博弈（包含状态空间，动作空间）    | 由异构信息网络派生出的复杂的环境，包含用户与项的交互记录 |
|  State（状态）  |                                          |                                                          |
|                 |              当前自己的手牌              |              一定范围内可推荐的电影候选集合              |
|                 |              对方手牌的联合              |                   其他用户的电影候选集                   |
|                 |        地主上家打出的所有牌的联合        |                用户搜索电影时输入的关键字                |
|                 |        地主下家打出的所有牌的联合        |             用户下一状态进行检索电影的关键字             |
|                 |            地主最近打出的动作            |                      用户的观影记录                      |
| Action（动作）  |                                          |                                                          |
|                 |             经过合法组合的牌             |                     推荐度较高的电影                     |
| Reward（奖励）  |                                          |                                                          |
|                 | 出完所有的牌则赢，reward=1，否则reward=0 |        如果用户点击了推荐项，提高该推荐项的偏重w         |





假设有一条卡牌游戏中的 trace: [(0, '8222'), (1, 'pass'), ('pass'), (0, '6KKK'), (1, 'pass'), (2, 'pass'), (0, '8'), (1, 'Q')]，元素：（用户ID，手牌组合）

**异构信息网络中**：

![image-20220520082605272](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202205200826333.png)



**卡牌游戏中**：

![image-20220520093706056](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202205200937098.png)



相邻节点：（landord、peasents、peasents）

> 1

​	在异构信息网络的推荐中，信息集（语料集）是一个   ==**多用户，多媒体，多文本**==   组成的信息网络。

映射到

​	卡牌游戏的推荐中，一场博弈：不同的player角色，不同的card组合  的信息网络

> 疑点

一场博弈中action中，往往是存在一定顺序的，相互之间存在影响。

电影推荐中，每个用户浏览电影的行为往往是独立的，相互之间不存在影响。









多个地主：

![image-20220520092641998](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202205200926044.png)

开题阶段：需要有实验数据进行开题

第三章：斗地主

第四章：异构网络
