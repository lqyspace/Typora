# 理论学习

强化学习从入门到入土，最全面的知识体系，大名鼎鼎的RLBook：[Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2020.pdf)

更通俗易懂的[Google DeepMind版本](https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf)

[从动态规划到近似动态规划](https://zhuanlan.zhihu.com/p/58837258)



## DQN

[起源：Nature DQN](https://www.nature.com/articles/nature14236)

### 研究内容：

#### 智能体改进：

- 网络结构

  [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461.pdf)

  [Dueling network architectures for deep reinforcement learning](https://arxiv.org/pdf/1511.06581.pdf)

  [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/pdf/1707.06887.pdf)

- 探索策略

  [Noisy Networks for Exploration](https://arxiv.org/pdf/1706.10295.pdf)

- 经验回放池

  [Prioritized Experience Replay](https://arxiv.org/pdf/1511.05952.pdf)

- 学习方式

  [Learning to predict by the methods of temporal differences](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf)（多步学习）

  [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783.pdf)（异步方法/AC）

  [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf)（多智能体学习/AC）

- ...

建议的实验方案：在多个游戏上，与改进前模型以及其他模型进行性能比较。（Atari）

baseline参考：

[openAI提供的强化学习baseline库](https://github.com/openai/baselines/)

[基于openAI baseline的改进实现](https://github.com/Stable-Baselines-Team/stable-baselines)

#### 问题建模：

[Gym使用简介](https://zhuanlan.zhihu.com/p/338428207)

[一个简单Gym示例](https://blog.csdn.net/qq_33446100/article/details/118249795)

- [ ] 环境状态：智能体能够获取的知识，应适当考虑所使用数据集的情况。状态中包含的知识信息越少，则智能体最终能达到的效果受限，或需要进行更久的训练。问题通常建模为可以被分解为若干个子问题的情况，环境状态空间即为子问题求解对象集。以新闻推荐举例，环境状态通常与用户表征及新闻表征向量相关，状态空间即为可以容纳所有用户和新闻表征向量的多维空间，状态即为有限个用户和新闻的表征向量集合。如下述DRN，状态为单用户表征+若干个候选新闻表征。

- [ ] 智能体动作：智能体的网络输出的是状态Q值，有的方法将遍历动作集寻找最大Q值对应动作，可以视为智能体的决策结果，即问题求解的答案，因此动作空间应能包含问题建模对应的全体解，动作对应当前子问题解。同样以新闻推荐举例，下述DRN求解推荐列表，因此动作空间为能容纳所有新闻表征向量的多维空间，动作为若干个新闻表征；预测点击率的情况，动作空间可以设为为[0,100%]的取值空间，动作为预测点击率值。

- [ ] 动作奖励：智能体学习的凭依，由环境反馈给出。由于动作为当前子问题解，因此动作奖励通常与后续的实验性能比较中，评价指标直接相关。大部分情况下动作奖励公式和评价指标的计算公式是高度一致的。

- [ ] ...

- 新闻推荐

  [综述]()

  - 单模态

    [DRN: A Deep Reinforcement Learning Framework for News Recommendation](https://dl.acm.org/doi/10.1145/3178876.3185994)

  - 多模态

    [MM-Rec: Multimodal News Recommendation](https://arxiv.org/pdf/2104.07407.pdf)

  数据集

  [MIcrosoft News Dataset (MIND)](https://msnews.github.io/)

  ...

  baseline参考：

  [微软recommenders](https://github.com/microsoft/recommenders)

  

- ...

  

建议的实验方案：同一个数据集上，与多种问题解决方案进行性能比较（不局限于强化学习，强化学习作为一种解决方案，与其它问题解决方案进行比较）



遇到过印象深刻的问题：

- 智能体不收敛/收敛极慢：尝试观察经验回放池中的经验，此问题通常发生于大部分经验样本都为正例/负例时。解决方案：如果机器性能支持，直接扩大经验回放池的Memory Buffer。如果始终无法平衡经验样本的正例/负例，尝试优化问题建模。
- ...