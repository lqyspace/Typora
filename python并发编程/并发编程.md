[TOC]
# 并发编程
## 并发编程简介
***
### 提升程序速度的方法

- 单线程穿行2
- 多线程并发
- 多CPU并发
- 多机器并行
 ![并发编程](D:/Boostnote%E7%AC%94%E8%AE%B0/img/python/2.png)

***

### python对并发编程的支持

![](D:/Boostnote笔记/img/python/3.png)

***

### python并发编程的三种选择

![](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202205192327010.png)

![](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202205192327014.png)

#### 什么是CPU密集型计算 IO密集型计算
![](D:/Boostnote笔记/img/python/6.png)

#### 多线程 多进程 多协程的对比
![](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202205192327035.png)

#### 怎么样根据任务选择对应的技术
![](D:/Boostnote笔记/img/python/8.png)

***
***
## python速度慢的罪魁祸首 全局解释器锁GIL
![](D:/Boostnote笔记/img/python/9.png)

### python速度慢的两大原因
![](D:/Boostnote笔记/img/python/10.png)

### GIL是什么
![](D:/Boostnote笔记/img/python/11.png)

### 为什么有GIL这个东西
![](D:/Boostnote笔记/img/python/12.png)

### 怎么样规避GIL带来的限制
![](D:/Boostnote笔记/img/python/13.png)

## python创建多线程的方法
![](D:/Boostnote笔记/img/python/14.png)

> blog_spider.py
```python
import requests

urls = [
    f"https://www.cnblogs.com/#p{page}"
    for page in range(1,50+1)
]

# print(urls)
def craw(url):
    r = requests.get(url)
    print(url,len(r.text))

# craw(urls[0])
```

> multi_thread_craw.py

```python
import blog_spider
import threading
import time

# 单线程执行
def single_thread():
    print("single_thread begin!")
    for url in blog_spider.urls:
        blog_spider.craw(url)
    print("single_thread end")


# 多线程执行过程中，线程是无序的
def multi_thread():
    print("multi_thread begin!")
    threads = []
    for url in blog_spider.urls:
      """
      本段代码的意义是：创建一个线程的列表，便于后续的利用，快速启动
      """
        threads.append(threading.Thread(target=blog_spider.craw, args=(url,))) # args的参数必须是元组

    for thread in threads:
      """
      本段代码的意义是：一次性启动线程列表中的所有线程
      """
        thread.start()

    for thread in threads:
      """
      本段代码的意义是：对所有线程进行阻塞，使他们在主线程之前结束
      """
        thread.join()
    print("multi_thread end!")


if __name__ == "__main__":
    """
    本段代码的意义是：计算单线程的运行时间
    """
    start = time.time()
    single_thread()
    end = time.time()
    print("single thread cost:", end - start, "seconds")


    """
    本段代码的意义是：计算多线程的运行时间
    """
    start = time.time()
    multi_thread()
    end = time.time()
    print("multi thread cost:", end - start, "seconds")
```

***
***

## python实现生产者消费者爬虫
![](D:/Boostnote笔记/img/python/15.png)

### 多组件的Pipline技术架构
![](D:/Boostnote笔记/img/python/16.png)

### 生产者消费者爬虫的架构
![](D:/Boostnote笔记/img/python/17.png)

### 多线程数据通信的queue.Queue
![](D:/Boostnote笔记/img/python/18.png)

    注意：
      q.put() 和 q.get() 在使用时是阻塞的，意思是说，对于 q.get() 而言，当队列里面没有内容时，线程会被卡住，
    直到获取到一个值则继续进行；对于 q.put() 而言，当队列里面的内容已经满的时候，线程会被卡住，直到有个空闲的
    位置put进去，则会继续进行。

### 代码编写实现生产者消费者爬虫

> blog_spider.py
```python

import requests
from bs4 import BeautifulSoup

# 声明一个超链接的列表：列表推导式
urls = [
  f"https://www.cnblogs.com/#p{page}"
  for page in range(1,50+1)
]

def craw(url):
  """
  :param url: url
  :return: 用于返回爬取的文本内容
  """
  r = requests.get(url)
  return r.text
  
  
def parse(html):
    """
    :param html: html
    :return: 返回文本中的(超链接,文本)的列表
    """
    # class="post-item-title"
    soup = BeautifulSoup(html,"html.parser")
    links = soup.find_all('a',class_="post-item-title")
    return [(link['href'],link.get_text()) for link in links]
    
if __name__ == "__main__":
    for result in parse(craw(urls[2])):
        print(result)
```

> 02_producer_consumer_spider.py

```python
"""
生产者消费者爬虫：

"""

import queue
import blog_spider
import threading
import time as tm
import random

# 生产者线程
def do_craw(url_queue:queue.Queue, html_queue:queue.Queue):
  """
  本段代码的意义是：从生产者队列中取出url，用于产生html，并将产生的html放进
html_queue队列中
  """
  while True:
    url = url_queue.get()
    html = blog_spider.craw(url)
    html_queue.put(html)
    
    print(threading.current_thread().name,f"craw {url}","url_queue.size=",url_queue.qsize())
    tm.sleep(random.randint(1,2))
    

# 消费者线程
def do_parse(html_queue:queue.Queue,fout):
  """
    本段代码的意义是：从html_queue队列中取出html，用于产生超链接和标题，并
把结果打印进fout文档中
  """
  while True:
    html = html_queue.get()
    results = blog_spider.parse(html)
    for result in results:
      fout.write(str(result)+'\n')
      
    print(threading.current_thread().name.f"results.size",len(result),"html_queue.size=",html_queue.qsize())
    tm.sleep(random.randint(1,2))
    

if __name__=="__main__":
  # Queue模块：先进先出FIFO
  url_queue = queue.Queue()
  html_queue = queue.Queue()
  for url in blog_spider.urls:
    """
    主线程将数据放进生产者队列中
    """
    url_queue.put(url)
    
  # 启动三个生产者线程
  for idx in range(3):
    t = threading.Thread(target=do_craw,args=(url_queue,html_queue),name=f"craw{idx}")
    t.start()
    
  # 添加两个消费者线程
  fout = open('02_data.txt', 'w', encoding='utf8')
  for idx in range(2):
    """
    消费者线程对中间的数据html_queue进行处理得到最终的产出
    """
    t = threading.Thread(target=do_parse, args=(html_queue, fout), name=f"parse{idx}")
    t.start()
```

***
***

## 线程安全以及Lock解决方案
![](D:/Boostnote笔记/img/python/19.png)

### 线程安全概念介绍
![](D:/Boostnote笔记/img/python/20.png)

### Lock用于解决线程安全的问题
![](D:/Boostnote笔记/img/python/21.png)

### 代码演示问题以及解决方案

> 无Lock状态：03_lock_concurrent.py

```python
import threading
import time as tm

class Account:
    def __init__(self,balance):
        self.balance = balance

def draw(account:Account,amount):
    if account.balance >= amount:
        tm.sleep(0.1) # 当前线程睡眠必然导致，线程切换
        print(threading.current_thread().name,"取钱成功")
        account.balance -= amount
        print(threading.current_thread().name,"当前余额",account.balance)
    else:
        print(threading.current_thread().name,"取钱失败，余额不足")


if __name__=="__main__":
    account = Account(1000)
    ta = threading.Thread(name="ta",target=draw,args=(account,800))
    tb = threading.Thread(name="tb",target=draw,args=(account,800))

    ta.start()
    tb.start()
  
  
--------------------输出结果--------------------------
ta 取钱成功
ta tb 取钱成功当前余额
tb  当前余额200
 -600
 
```

> Lock状态：03_lock_concurrent.py

```python
import threading
import time as tm

lock = threading.Lock()

class Account:
    def __init__(self,balance):
        self.balance = balance

def draw(account:Account,amount):
  """
  当加上一个lock之后，在执行第一个线程的时候，虽然会遇到sleep函数导致线程切换到第二个，
但是第一个线程被锁定，还未被释放，第二个线程无法获得锁，所以第二个线程无法执行。当一个线程
等待0.1秒后继续执行第一个线程的操作，直到第一个线程释放锁，第二个线程才能获得锁而继续执行
  """
    with lock:
        if account.balance >= amount:
            tm.sleep(0.1) # 当前线程睡眠必然导致，线程切换
            print(threading.current_thread().name,"取钱成功")
            account.balance -= amount
            print(threading.current_thread().name,"当前余额",account.balance)
        else:
            print(threading.current_thread().name,"取钱失败，余额不足")


if __name__=="__main__":
    account = Account(1000)
    ta = threading.Thread(name="ta",target=draw,args=(account,800))
    tb = threading.Thread(name="tb",target=draw,args=(account,800))

    ta.start()
    tb.start()
    
--------------------输出结果--------------------------
ta 取钱成功
ta 当前余额 200
tb 取钱失败，余额不足
```
***

## python 好用的线程池ThreadPoolExecuter

![](D:/Boostnote笔记/img/python/22.png)

### 线程池的原理

![](D:/Boostnote笔记/img/python/23.png)

```python
"""
解释说明：

  一个线程的生命周期：
  
          新建一个线程的时候，这个线程处于完全不动的状态，然后我们调用start方法，它会进入就绪的状态，这个时候并没有真正的运行，
      因为一个线程的运行是需要系统进行调度的，系统进行调度，让这个线程获得cpu资源，它才进入运行状态。在运行的过程中，它可能会
      再次失去cpu资源，进入就绪的状态。它也有可能因为自身遇到了sleep/io进入阻塞的状态，当对应的sleep/io完成了以后，阻塞状
      态会再次回到了就绪状态等待系统的调度。当run方法执行完了以后，或者线程被终止，就会进入终止的状态。
      
      
      
      
      
  一个线程池主要包括两个部分：任务队列、线程池
  
          线程池里面是提前建好的线程，这些线程会被重复的使用。同时还有一个任务队列的概念，当一个新的任务来了之后，并不会新建一个
      线程，而是放到任务队列中。线程池中的线程会挨个取任务队列中的任务，当一个任务执行完了会取出下一个任务进行执行。或者说发现没有
      任务，它就会回到线程池并不销毁线程池中的线程，放在线程池里等待下一个任务的到来。
          通过一个任务队列以及一个可重用的线程，就实现了线程池的这样的功能。
         
         
"""
```

### 使用线程池的好处

![](D:/Boostnote笔记/img/python/24.png)

### ThreadPoolExecutor 的使用用法
![](D:/Boostnote笔记/img/python/25.png)

```python
"""
第一种方法：pool.map(craw,urls)中传入的是一个函数名和一个参数列表，参数列表是将所有所有的参数传进去
           map映射的结果是和参数urls的顺序对应的

第二种方法：pool.submit(craw,url)中传的参数是一个具体的参数url，并非urls。
           对结果有两种遍历的方式：
            1、for future in futures:
                  print(future.result())
              它是按照url定义的顺序挨个输出。
            2、for future in as_completed(futures):
                  print(future.result())
              使用该方法的好处是，不管里面的那个任务先执行完了，就会先进行返回。而第一种方式，它
            会挨个等待按顺序的url的结果进行返回和打印
"""
```

### 使用线程池改造爬虫程序

> blog_spider.py 部分与上面的一样

> 04_thread_pool.py

```python
"""
描述：使用线程池改造爬虫程序
"""
from concurrent.futures import ThreadPoolExecutor,as_completed
import blog_spider

# craw
with ThreadPoolExecutor() as pool:
    htmls = pool.map(blog_spider.craw,blog_spider.urls)
    htmls = list(zip(blog_spider.urls,htmls))
    for url,html in htmls:
        print(url,len(html))
# exit()
# parse
with ThreadPoolExecutor() as pool:
    futures = {}
    for url,html in htmls:
        future = pool.submit(blog_spider.parse,html)
        futures[future] = url

    for future,url in futures.items():
        print(url,future.result())

    # for future in as_completed(futures):
    #     url = futures[future]
    #     print(url,future.result())
```

## 线程池在web服务中实现加速

![](D:/Boostnote笔记/img/python/26.png)

### web服务的架构以及特点

![](D:/Boostnote笔记/img/python/27.png)

### 使用线程池ThreadPoolExecutor加速

![](D:/Boostnote笔记/img/python/28.png)

### 代码实现web服务的加速

```python
"""
描述：线程池加速web服务
"""
import flask
import json
import time as tm
from concurrent.futures import ThreadPoolExecutor

# 在全局声明一个线程池
pool = ThreadPoolExecutor()

app = flask.Flask(__name__)


def read_file():
    tm.sleep(0.1)
    return "file result"


def read_db():
    tm.sleep(0.2)
    return "db result"


def read_api():
    tm.sleep(0.3)
    return "api result"


@app.route("/")
def index():
    """
    在此处对三个函数加上线程池
    """
    result_file = pool.submit(read_file)
    result_db = pool.submit(read_db)
    result_api = pool.submit(read_api)  # 快捷键：Alt+Enter 弹出菜单

    return json.dumps({
        "result_file": result_file.result(),
        "result_db": result_db.result(),
        "result_api": result_api.result()
    })


if __name__ == "__main__":
    app.run()
    
```

## 使用多进程multiprocessing加速程序的运行

![](D:/Boostnote笔记/img/python/29.png)

### 为什么还要使用多进程multiprocessing模块

![](D:/Boostnote笔记/img/python/30.png)

### 对进程multiprocessing知识梳理
![](D:/Boostnote笔记/img/python/31.png)

### 单线程，多线程，多进程对比cpu密集型计算

![](D:/Boostnote笔记/img/python/32.png)

### 代码实现

```python
"""
描述：cpu密集型计算
"""
import math
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

import time as tm

PRIMES = [112272535095293] * 100


def is_prime(n):
    if n < 2:
        return False
    if n == 2:
        return True
    if n % 2 == 0:
        return False
    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True


def single_thread():
    for number in PRIMES:
        is_prime(number)


def multi_thread():
    with ThreadPoolExecutor() as pool:
        pool.map(is_prime, PRIMES)


def multi_process():
    with ProcessPoolExecutor() as pool:
        pool.map(is_prime, PRIMES)


if __name__ == "__main__":
    start = tm.time()
    single_thread()
    end = tm.time()
    print("single_thread cost:", end - start)

    start = tm.time()
    multi_thread()
    end = tm.time()
    print("multi_thread cost:", end - start)

    start = tm.time()
    multi_process()
    end = tm.time()
    print("multi_process cost:", end - start)


-----------------------输出结果--------------------------
single_thread cost: 59.808218002319336
multi_thread cost: 62.42924189567566
multi_process cost: 27.27966046333313
```

## 在flask服务中使用进程池加速

```python
"""
描述：flask进程加速
"""
import flask
from concurrent.futures import ProcessPoolExecutor
import math
import json


app = flask.Flask(__name__)

def is_prime(n):
    if n < 2:
        return False
    if n == 2:
        return True
    if n % 2 == 0:
        return False
    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True

@app.route("/is_prime/<numbers>")
def api_is_prime(numbers):
    number_list = [int(x) for x in numbers.split(',')]
    results = process_pool.map(is_prime,number_list)
    return json.dumps(dict(zip(number_list,results)))


if __name__=="__main__":
    process_pool = ProcessPoolExecutor()
    app.run()
```

## python 异步IO实现并发爬虫

### 协程

![](D:/Boostnote笔记/img/python/33.png)

![](D:/Boostnote笔记/img/python/34.png)

### 代码实现协程异步IO

```python
"""
描述：协程：单线程异步IO爬虫
"""
import asyncio
import aiohttp
import blog_spider
import time


# 定义一个协程函数
async def async_craw(url):
    print("craw url:",url)
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            result = await resp.text()
            print(f"craw url: {url}, {len(result)}")

# 获取事件循环
loop = asyncio.get_event_loop()

tasks = [loop.create_task(async_craw(url)) for url in blog_spider.urls]

start = time.time()
loop.run_until_complete(asyncio.wait(tasks))
end = time.time()
print("use time seconds:",end-start)


-----------输出结果-------------
....
craw url: https://www.cnblogs.com/sitehome/p/45, 70679
craw url: https://www.cnblogs.com/sitehome/p/35, 70044
craw url: https://www.cnblogs.com/sitehome/p/3, 69699
use time seconds: 0.5364642143249512


```

## 在异步IO中使用信号量控制爬虫并发度

### 信号量

![](D:/Boostnote笔记/img/python/35.png)

### 代码实现

```python
"""
描述：协程：单线程异步IO爬虫，信号量控制
"""
import asyncio
import aiohttp
import blog_spider
import time

# 初始化一个信号量
semaphore = asyncio.Semaphore(10)


# 定义一个协程函数
async def async_craw(url):
    async with semaphore:
        print("craw url:",url)
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as resp:
                result = await resp.text()
                await asyncio.sleep(5)
                print(f"craw url: {url}, {len(result)}")

# 获取事件循环
loop = asyncio.get_event_loop()

tasks = [loop.create_task(async_craw(url)) for url in blog_spider.urls]

start = time.time()
loop.run_until_complete(asyncio.wait(tasks))
end = time.time()
print("use time seconds:",end-start)


--------输出结果----------
...
..............s://www.cnblogs.com/sitehome/p/48
craw url: https://www.cnblogs.com/sitehome/p/49
craw url: https://www.cnblogs.com/sitehome/p/50
craw url: https://www.cnblogs.com/sitehome/p/41, 70311
craw url: https://www.cnblogs.com/sitehome/p/42, 70092
craw url: https://www.cnblogs.com/sitehome/p/43, 71057
craw url: https://www.cnblogs.com/sitehome/p/44, 69806
craw url: https://www.cnblogs.com/sitehome/p/46, 69974
craw url: https://www.cnblogs.com/sitehome/p/45, 70901
craw url: https://www.cnblogs.com/sitehome/p/49, 70222
craw url: https://www.cnblogs.com/sitehome/p/50, 70022
craw url: https://www.cnblogs.com/sitehome/p/48, 70718
craw url: https://www.cnblogs.com/sitehome/p/47, 70024
use time seconds: 27.250171422958374
```

## 使用subprocess播放歌曲解压文件

### 使用subprocess启动电脑子进程
![](D:/Boostnote笔记/img/python/36.png)

![](D:/Boostnote笔记/img/python/37.png)