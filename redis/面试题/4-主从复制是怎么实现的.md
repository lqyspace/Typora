# 主从复制是怎么实现的

我们已经了解到AOF和RDB可以使数据持久化到磁盘上，保证了即使在服务器重启的情况下也不会有数据丢失（或少量丢失）

不过，由于服务器是存储在一台服务器上的，如果服务器出事了就完犊子了，比如：

- 如果服务器出现宕机，由于数据恢复是需要时间的，因此这段时间内是不能够处理新的请求的
- 如果这台服务器的磁盘出现了故障，可能数据就都丢失了

要避免这种单点故障，最好的办法就是把数据复制到其他的服务器上，让这些服务器也可以对外提供服务，这样即使有一台服务器出现了故障，也能保证其他服务器可以继续对外提供服务。

![图片](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202307262134693.png)

多台服务器要保存同一份数据，那么问题就来了。

如何保证这些服务器之间的数据一致性，数据的读写操作是否每台服务器都可以处理？

Redis提供了**主从复制模式**，来避免上述的问题。

这个模式可以保证多台服务器之间的数据一致性，且主从服务器之间采用的是 **读写分离** 的方式。

主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来的些操作命令，然后执行这条命令。

![图片](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202307262349375.png)

也就是说，所有的数据修改只在主服务器上进行，然后将最新的数据同步给从服务器，这样就使得主从服务器的数据是一致的。

同步这两个字说得简单，但是这个同步过程并没有那么简单，要考虑的事情不是一两个。

我们先来看看主从服务器的第一次同步是如何工作的？



## 第一次同步

多台服务器之间要通过什么方式确定谁是主服务器，谁是从服务器呢？

我们可以使用 `replicaof`（Redis5.0之前使用 `slaveof`）命令形成主服务器和从服务器的关系。

比如，现在有服务器A和服务器B，我们在服务器B上执行下面这条命令：

```shell
# 服务器B执行这条命令
replicaof <服务器A的ip地址> <服务器A的Redis端口号>
```

接着，服务器B就会变成服务器A的从服务器，然后与主服务器进行第一次同步。

主从服务器间的第一次同步过程可分为三个阶段：

- 第一阶段是建立链接，协商同步
- 第二阶段是主服务器同步数据给从服务器
- 第三阶段是主服务器发送新写数据命令给从服务器

![图片](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202307262357410.png)

接下来我们具体介绍每一个阶段都做什么。

**第一阶段：建立连接，协商同步**

执行了replicaof命令后，从服务器就会给主服务器发送 `psync`命令，表示要进行数据同步。

psync命令包含两个参数，分别是**主服务器的runID** 和 **复制进度 offset**。

- runID，每个Redis服务器在启动时都会自动生成一个随机的ID来唯一标识自己。当从服务器和主服务器第一次同步时，因为不知道主服务器的runID，所以将其设置为 "?"
- offset，表示复制的进度，第一次同步时，其值为 -1

主服务器收到 `psync`后，会用 `FULLRESYNC`作为响应命令返回给对方。

并且这个响应命令会带上两个参数：主服务器的runID和主服务器目前的复制进度offset。从服务器收到响应后，会记录着两个值。

`FULLRESYNC`响应命令的意图是采用 ==全量复制==的方式，也就是主服务器会把所有数据都同步给从服务器。

所以第一阶段的工作是为了 **全量复制** 做准备。

那么具体怎么进行 **全量同步** 呢？我们可以看第二阶段。



**第二阶段：主服务器同步数据给从服务器**

接着，主服务会执行 `bgsave` 命令来生成RDB文件，然后把文件发送给从服务器。

从服务器收到RDB文件以后，会先清空当前的数据，然后载入RDB文件。

这里有一点要注意，主服务是不会阻塞主进程的，因为 `bgsave` 命令是产生了一个子进程来做生成RDB文件的工作，是异步工作，这样Redis依旧可以处理处理客户端发来的请求。

但是，这期间的写操作并没有记录到刚刚生成的RDB文件中，这时主从服务器间的数据就不一致了。

那么为了保证主从服务器间的数据一致性，==主服务器在下面这三个时间间隙中将收到的写操作命令，写入到replication buffer缓冲区里：==

- 主服务生成RDB文件期间
- 主服务器发送RDB文件给从服务器期间
- 从服务器加载RDB文件期间



**第三阶段：主服务器发送新写操作命令给从服务器**

在主服务器生成的RDB文件发送完，从服务器收到RDB文件后，丢弃所有的旧数据，将RDB数据载入到内存中。完成RDB的载入后，会回复一个确认消息给主服务器。

接着，主服务器将replication buffer缓冲区里所记录的写操作命令发送给从服务器，从服务器执行来自主服务器replication buffer缓冲区里发来的命令，这时主从服务器的数据就一致了。

至此，主从服务器的第一次同步工作执行完成。



## 命令传播

主从服务器在完成第一次同步后，双方之间就会维护一个TCP连接。

![图片](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202307270028945.png)

后续主服务器可以通过这个链接继续将写操作命令传递给从服务器，然后从服务器执行该命令，使得与主服务器的数据库状态相同。

而且这个链接是长连接的，目的是避免频繁地进行tcp连接和断开带来的性能开销。

上面的这个过程被称为==基于长连接的命令传播==，通过这种方式来保证第一次同步后的主从服务器的数据一致性。



## 分摊主服务器的压力

在前面的分析中，我们可以知道主从服务器在第一次数据同步的过程中，主服务器会做两件耗时的操作：生成RDB文件和传输RDB文件。

主服务器是可以有多个从服务器的，如果从服务器数量非常多，而且都与主服务器进行全量同步的话，就会带来两个问题：

- 由于是通过bgsave来生成RDB文件，那么主服务器就会忙于使用fork()创建子线程，如果主服务器的内存数据非常大，在执行fork()函数时是会阻塞主线程的，从而使得Redis无法正常处理请求。
- 传输RDB文件会占用主服务器的网络带宽，会对主服务器响应命令请求产生影响。

这种情况就好像，刚创业的公司，由于人不多，所以员工都归老板一个人管，但是随着公司的发展，人员的扩充，老板慢慢就无法承担全部员工的管理工作了。

**要解决这个问题，老板就需要设立经理职位，由经理管理多名普通员工，然后老板只需要管理经理就好。**

Redis也是一样的，从服务器可以有自己的从服务器，我们可以把拥有从服务器的从服务器作为经理角色，它不仅可以接受主服务器的同步数据，自己也可以同时作为主服务器的形式将数据同步给从服务器，组织形式如下图：

![图片](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202307270040290.png)

通过这种形式，==主服务器生成RDB和传输RDB的压力可以分摊给充当经理角色的从服务器。==

那具体怎么做到的呢？

其实很简单，我们可以在 从服务器 上执行下面这条命令，使其作为目标服务器的从服务器：

```shell
replicaof <目标服务器的ip> 6379
```

此时如果目标服务器本身也是从服务器，那么该目标服务器就会成为经理角色，不仅可以接受主服务器同步过来的数据，也会把数据同步给自己旗下的从服务器，从而减轻主服务器的负担。





## 增量复制

主从服务器在完成第一次同步后，就会基于长连接进行命令传播。

可是网络总是不按套路出牌，说延迟就延迟，说断开就断开。

如果主从服务器间的网络连接断开了，那么就无法进行命令传播了，这是从服务器的数据就没办法和主服务器保持一致了，客户端就可能从 **从服务器** 读到旧的数据。

![图片](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202307270048125.png)

那么问题就来了，如果此时断开的网络，又恢复正常了，要怎么保证主从服务器的数据一致性呢？

在Redis2.8之前，如果主从服务器在命令同步时出现了网络断开又恢复的现象，从服务器就会和主服务器重新进行一次全量复制，很明显这样的开销太大了，必须要进行改进一波。

所以，在Redis2.8开始，网络断开又恢复后，主从服务器会采用==增量复制==的方式继续同步，也就是只会把网络断开期间主服务器接受的写操作命令，同步给从服务器。

网络恢复后的增量复制过程如下图：

![图片](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202307270130978.png)

主要有三个步骤：

- 从服务器在恢复网络后，会发送 `psync` 命令给主服务器，此时的 `psync` 命令里的offset参数不是 -1
- 主服务器收到该命令后，然后用CONTINUE响应命令告诉从服务器接下来采用增量复制的方式同步数据
- 然后主服务器将主从服务器断线期间，所执行的写命令发送给从服务器，然后从服务器执行这些命令

那么问题的关键来了，==主服务器如何辨别要将哪些增量数据发送给从服务器呢？==

答案藏在这两个东西里：

- repl_backlog_buffer，是一个环形缓冲区，用于主从服务器断连后，从中找到差异的数据。
- replication offset，标记上面那个缓冲区的同步进度，主从服务器都有各自的偏移量，主服务器使用master_repl_offset来记录自己写到的位置，从服务器使用slave_repl_offset来记录自己读到的位置。

那repl_backlog_buffer缓冲区是什么时候写入的呢？



在主服务器进行命令传播时，不仅会将写命令发送给从服务器，还会将写命令写入到repl_backlog_buffer缓冲区里，因此这个缓冲区里会保存着最近传播的写命令。

网络断开后，当从服务器重新连上主服务器时从服务器会通过`psync`命令将自己的复制偏移量slave_repl_offset发送给主服务器，主服务器根据自己的master_repl_offset和slave_repl_offset之间的差距，然后来决定对从服务器执行哪种同步操作：

- 如果判断出从服务器要读取的数据还在repl_backlog_buffer缓冲区里，那么主服务器将采用==增量同步==的方式；
- 相反，如果判断出从服务器要读取的数据已经不在repl_backlog_buffer缓冲区里，那么主服务器将采用==全量复制==的方式；

当主服务器在repl_backlog_buffer中找到主从服务器差异（增量）的数据后，就会将增量的数据写入到replication buffer缓冲区中，这个缓冲区前面我们也提到过，它是缓存将要传播给从服务器的命令。

![图片](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202307270157089.png)

repl_backlog_buffer缓冲区的默认大小是1M，并且由于它是一个环形缓冲区，所以当缓冲区写满后，主服务器继续写入的话，就会覆盖之前的数据。因此，当主服务器的写入速度远超过从服务器的读取速度时，缓冲区的数据一下就会被覆盖。

那么在网络恢复时，从服务器想读取的数据已经被覆盖，主服务器就会采用全量同步，这个方式比增量同步的性能损耗要大得多。

因此，为了避免在网络恢复时，主服务器频繁地使用全量同步的方式，我们应该调整下repl_backlog_buffer缓冲区大小，尽可能地大一些，减少出现从服务器要读取的数据被覆盖的概率，从而使得主服务器采用增量同步的方式。

那repl_backlog_buffer缓冲区的大小调整到多少合适呢？

repl_backlog_buffer最小的大小可以根据下面这个公式进行推算。

![图片](https://raw.githubusercontent.com/lqyspace/mypic/master/PicBed/202307270203291.png)

这个公式的意思：

- second为从服务器断线后重新连上主服务器所需的平均时间（以秒计算）
- write_size_per_second则是主服务器平均每秒产生的写命令数据量的大小

举个例子，如果主服务器平均每秒产生1MB的写命令，而从服务器断连后需要5秒才能重新连上主服务器。

那么repl_backlog_buffer大小就不能低于5MB，否则新写的命令就会覆盖旧数据。

当然，为了应对一些突发情况，可以将repl_backlog_buffer的大小设置为此基础的2倍，也就是10MB。

关于repl_backlog_buffer大小修改的方法，只需要修改配置文件里下面这个参数的值即可：

```shell
repl-backlog-size 1mb
```



## 总结

主从复制共有三种模式：全量复制、基于长连接的命令传播，增量复制

主从服务器第一次同步的时候，就是采用全量复制，此时服务器会有两个比较耗时的操作，分别是生成RDB文件和传输RDB文件。为了避免过多的从服务器和主服务器进行全量复制，可以把一部分从服务器升级为经理角色，让它也有自己的从服务器，通过这样可以分摊主服务器的压力。

第一次同步完成后，主从服务器都会维护一个长链接，主服务器在接受到一个写操作命令后，就会通过这个链接将写命令同步给从服务器，来保证主从服务器的数据一致性。

如果遇到网络断开，增量复制就上场了，不过这个还跟repl_backlog_buffer的大小有关系。

如果它配置的过小，主从服务器网络恢复时，可能发生从服务器想读的数据已经被覆盖的情况，那么这时就会导致主服务器采用全量复制的方式，所以为了避免这种情况的频繁发生，要调大这个参数的值，以降低主从服务器断开后全量同步的概率。



# 面试题

## redis主从节点是长链接还是短连接

长链接



## 怎么判断Redis某个节点是否正常工作

Redis判断节点是否正常工作，基本上都是通过互相的ping-pong心跳检测，如果有一半以上的节点去ping一个节点的时候没有pong回应，集群就会认为这个节点挂掉了，会断开与这个节点的连接。

Redis主从节点发送的心态间隔是不一样的，而且作用也有一点区别：

- Redis主节点默认每隔10秒对从节点发送ping命令，判断从节点的存活性和连接状态，可通过参数repl_ping_slave_period控制发送频率
- Redis从节点每隔一秒发送replconf ack{offset}命令，给主节点上报自身当前的复制偏移量，目的是为了：
  - 实时监测主从节点网络状态
  - 上报自身复制偏移量，检查复制数据是否丢失，如果从节点数据丢失，再从主节点的复制缓冲区中拉取丢失数据。





## 主从复制架构中，过期key如何处理

主节点处理一个key或者通过淘汰算法淘汰了一个key，这个时间主节点模拟一条del命令发送给从节点，从节点收到该命令后，就进行删除操作。



## Redis是同步复制还是异步复制

Redis主节点每次收到写命令之后，先写到内部的缓冲区，然后异步发送给从节点



## 主从复制中两个buffer（replication buffer、repl_backlog_buffer）有什么区别

replication buffer、repl_backlog_buffer区别如下：

- 出现的阶段不一样
  - repl_backlog_buffer是在**增量复制**阶段出现，==一个主节点只分配一个repl_backlog_buffer==；
  - replication buffer：在**全量复制阶段和增量复制**阶段都会出现，==主节点会给每个新链接的从节点，分配一个replication buffer。==
- 这两个buffer都有大小限制，当缓冲区满了以后，发生的事情不一样：
  - 当repl_backlog_buffer满了，因为是环形结构，**会直接覆盖起始位置的数据。**
  - 当replication buffer满了，会导致连接断开，删除缓存，从节点重新连接，**重新开始全量复制**



## 如何应对主从数据不一致？

> **为什么会出现主从数据不一致**

主从数据不一致，就是至客户端从从节点读取的值和主节点中的最新值并不一致。

之所以会出现主从数据不一致的现象，是**因为主从节点间的的命令复制是异步进行的**，所以无法实现强一致性保证（主从数据时时刻刻保持一致）。

具体来说，在主从节点命令传播阶段，主节点收到新的写命令后，会发送给从节点。但是，主节点并不会等到从节点实际执行完命令后，再把结果返回给客户端，而是主节点自己在本地执行完命令后，就会向客户端返回结果了。如果从节点还没有执行主节点同步过来的命令，那么主从节点之间的数据就不一致了。



> **如何应对主从数据不一致**

第一种方法，尽量保持主从节点间的网络连接状况良好，避免主从节点在不同的机房。

第二种方法，可以开发一个外部程序来监控主从节点件的复制进度。具体做法：

- Redis的INFO replication命令可以查看主节点接受写命令的进度信息（master_repl_offset）和从节点复制写命令的进度信息（slave_repl_offset），所以，我们就可以开发一个监控程序，先用INFO replication命令查到主，从节点的进度，然后，我们用master_repl_offset减去slave_repl_offset，这样就能得到从节点和主节点件的复制进度差值了。
- 如果某个从节点的进度差值大于我们设预设的阈值，我们可以让客户端不再和这个从节点连接进行数据读取，这样就可以减少读到不一致数据的情况。不过，为了避免客户端和所有的从节点都不能连接的状况，我们需要把复制进度差值的阈值设置的大一些。





## 主从切换如何减少数据丢失

主从切换的过程中，产生的数据丢失有两种：

- 异步复制同步丢失
- 集群产生脑裂数据丢失

我们不可能保证数据完全不丢失，只能做到使得尽量少的数据丢失。



> **异步复制同步丢失**

对于Redis主节点与从节点之间的数据复制，是异步复制。当客户端发送写请求给主节点的时候，客户端会返回ok，接着主节点将写请求异步同步给各个从节点，但是如果此时主节点还没来得及同步给从节点时发生了断电，那么主节点内存中的数据会丢失。



**减少异步复制的数据丢失的方案**

Redis配置里有一个参数 `min-slaves-max-lag`，表示一旦所有的从节点数据复制和同步的延迟都超过了 `min-slaves-max-lag`定义的值，那么主节点就会拒绝接受任何请求。

假设将 `min-slaves-max-lag`配置为10s，根据目前master->slave的复制速度，如果数据同步完成所需时间超过10s，就会认为master未来宕机后损失的数据会很多，master就拒绝写入新的请求。这样就能将master和slave的数据差控制在10s内，即使master宕机也只是这未复制的10s数据。

那么对于客户端，当客户端发现master不可写入时，我们可以采取降级措施，将数据暂时写入本地缓存和磁盘中，在一段时间（等master恢复正常）后重新写入master来保证数据不丢失，也可以将数据写入kafka消息队列，等master恢复正常，再隔一段时间取消费kafka中的数据，将数据重新写入master。



> **集群产生脑裂数据丢失**

先来理解集群的脑裂现象，这就好比一个人有两个大脑，那么到底受谁控制呢？

在Redis中，集群脑裂产生数据丢失的现象是怎么样的呢？

在Redis主从架构中，部署方式一般是 **一主多从**，主节点提供写操作，从节点提供读操作。

如果主节点的网络突然发生了问题，它与所有的从节点都失联了，但是此时的主节点和客户端的网络是正常的，这个客户端并不知道Redis内部出现了问题，还在照样的向这个失联的主节点写数据（过程A），此时这些数据被主节点缓存到了缓冲区中，因为主从节点之间的网络问题，这些数据都是无法同步给从节点的。

这是，哨兵也发现主节点失联了，它就认为主节点挂了（但实际上主节点正常运行，只是网络出现了问题），于是哨兵就会从从节点中选举出一个leader作为主节点，这是集群就有两个主节点了——脑裂现象。

这时候网络又好了，哨兵因为之前已经选举出了一个新的主节点，因此就会把旧的主节点降级为从节点（A），然后从节点就会向新主节点请求数据同步，==因为第一次同步是全量同步的方式，此时的从节点（A）就会清空自己本地的数据，然后再做全量同步。所以，之前客户端在过程A写入的数据就会丢失，也就是集群产生脑裂数据丢失的问题。==

总结一句话就是：由于网络问题，集群节点之间失去联系，主从数据不同步；重新平衡选举，产生两个主节点，等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失。



**减少脑裂的数据丢失的方案**

当主节点发现 **从节点下线的数量太多**，或者 **网络延迟太大** 的时候，那么主节点会禁止写操作，直接把错误返回给客户端。

在Redis的配置文件中有两个参数我们可以设置：

- min-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会禁止写操作
- min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果主从同步的延迟超过 x 秒，主节点会禁止写数据。

我们可以把 min-slaves-to-write和min-slaves-max-lag这两个配置项搭配起来使用，分别给他们设置一定的阈值，假设为N和T。



这两个配置项组合后的要求是，==主节点连接的从节点至少要有N个从节点，并且，主节点进行数据复制时的ACK消息延迟不能超过T秒，否则主节点就不会再接受客户端的请求了。==

即使原主节点是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从节点进行同步，自然也就无法和从节点进行ACK确认。这样一来，min-slaves-to-write和min-slaves-max-lag的组合要求就无法得到满足。==原主节点就会被限制接收客户端写请求，客户端也就不能在原主节点中写入数据了。==

==等到新主节点上线时，就只有新主节点能接收和处理客户端的请求，此时，新写的数据会被直接写到新主节点上。而原主节点会被哨兵降级为从节点，即使它的数据被清空了，也不会有新数据丢失。==

假设我们将min-slaves-to-write设置为1，把min-slaves-max-lag设置为12s，把哨兵的down-after-milliseconds设置为10s，主节点因为某些原因卡住了15s，导致哨兵判断主节点客观下线，开始进行主从切换。同时，因为原主节点卡住了15s，没有一个从节点能和原主节点在12s内进行数据复制，原主节点也无法接受客户端的请求，这样一来，主从切换完成后，也只有新主节点能接收请求，不会发生脑裂，也就不会发生数据丢失。





## 主从如何做到故障自动切换

主节点挂了，从节点是无法自动升级为主节点的，这个过程需要人工处理，在此期间Redis无法对外提供写操作。

此时，Redis哨兵就登场了，哨兵在发现主节点出现故障时，由哨兵自动完成故障发现和故障转移，并通知给应用方，从而实现高可用性。















































